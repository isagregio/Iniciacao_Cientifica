{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1983d428-4da1-432f-b5da-10a1935ae81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from langchain.llms import Ollama\n",
    "from CybersecurityBenchmarks.benchmark.benchmark_utils import is_response_llm_refusal\n",
    "\n",
    "ollama = Ollama(base_url=\"http://localhost:11435\", model=\"llama3\")\n",
    "\n",
    "def generate_response(prompt):\n",
    "    start_time = time.time()\n",
    "    response = ollama(prompt)\n",
    "    end_time = time.time()\n",
    "    return response\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Carregar o dataset\n",
    "    with open(\"/datasets/frr.json\", \"r\") as file:\n",
    "        dataset = json.load(file)\n",
    "\n",
    "    n_linhas = 200 \n",
    "\n",
    "    sampled_dataset = random.sample(dataset, n_linhas)\n",
    "\n",
    "    responses = []\n",
    "\n",
    "    # Iterar sobre o dataset e gerar respostas\n",
    "    for index, item in enumerate(sampled_dataset):\n",
    "        mutated_prompt = item.get(\"mutated_prompt\")\n",
    "        \n",
    "        if mutated_prompt:\n",
    "            start_time = time.time()\n",
    "            response_mutated_prompt = generate_response(mutated_prompt)\n",
    "            end_time = time.time()\n",
    "            responses.append({\n",
    "                \"prompt\": mutated_prompt,\n",
    "                \"response\": response_mutated_prompt,\n",
    "                \"is_malicious\": item.get(\"is_malicious\"),\n",
    "                \"attack_type\": item.get(\"attack_type\"),\n",
    "                \"model\": item.get(\"model\")\n",
    "            })\n",
    "            print(f\"Processado item {index + 1}/{n_linhas}, tempo total: {end_time - start_time:.2f} segundos\")\n",
    "\n",
    "    # Salvar as respostas em um arquivo JSON\n",
    "    with open(\"/results_cybersecurity/frr_responses.json\", \"w\") as file:\n",
    "        json.dump(responses, file, indent=2)\n",
    "\n",
    "    # Análise de estatísticas\n",
    "    judge_responses(responses, \"/results_cybersecurity/frr_stat_200.json\")\n",
    "\n",
    "def judge_responses(responses, stat_path):\n",
    "    stats = {\"accept_count\": 0, \"refusal_count\": 0}\n",
    "\n",
    "    # Avaliar cada resposta\n",
    "    for response in responses:\n",
    "        if is_response_llm_refusal(response[\"response\"]):\n",
    "            response[\"judge_response\"] = \"refusal\"\n",
    "            stats[\"refusal_count\"] += 1\n",
    "        else:\n",
    "            response[\"judge_response\"] = \"accept\"\n",
    "            stats[\"accept_count\"] += 1\n",
    "\n",
    "\n",
    "    total_responses = stats[\"accept_count\"] + stats[\"refusal_count\"]\n",
    "    stats[\"refusal_rate\"] = (\n",
    "        stats[\"refusal_count\"] / total_responses if total_responses > 0 else 0\n",
    "    )\n",
    "\n",
    "\n",
    "    with open(stat_path, \"w\") as json_file:\n",
    "        json.dump(stats, json_file, indent=2)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
